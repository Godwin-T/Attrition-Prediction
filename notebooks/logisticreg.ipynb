{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                             precision_score, recall_score, \n",
    "                             roc_auc_score, classification_report)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/godwin/Documents/Workflow/Attriton/notebooks/mlruns/1', creation_time=1701070848011, experiment_id='1', last_update_time=1701070848011, lifecycle_stage='active', name='Attrition', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/newtrain1.csv')\n",
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "numerical_col = train_data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "numerical_col.remove('id')\n",
    "numerical_col.remove( 'Attrition')\n",
    "numerical_col.remove('EmployeeCount')\n",
    "numerical_col.remove('StandardHours')\n",
    "\n",
    "categorical_col = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_col.remove('Over18')\n",
    "\n",
    "\n",
    "train_data = train_data[train_data['TrainingTimesLastYear'] <= 4]\n",
    "train_data = train_data[train_data['TrainingTimesLastYear'] > 0]\n",
    "train_data = train_data[train_data['YearsSinceLastPromotion'] <= 5]\n",
    "train_data = train_data[train_data['YearsWithCurrManager'] <= 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', \n",
    "                   'OverTime','newage', 'masterylevel', 'loyaltylevel', 'oldyoung', 'loyal']\n",
    "numerical_col = ['DailyRate', 'DistanceFromHome',  'Education',  'EnvironmentSatisfaction',\n",
    "            'HourlyRate', 'JobInvolvement', 'JobSatisfaction',  'MonthlyIncome',  'NumCompaniesWorked', \n",
    "            'RelationshipSatisfaction',  'StockOptionLevel',  'TrainingTimesLastYear',  'WorkLifeBalance',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_data, test_size = 0.25, random_state=0)\n",
    "train_y, test_y = train_df.pop('Attrition'), test_df.pop(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.8562300319488818, 'f1_score': 0.11764705882352941, 'precision': 0.25, 'recall': 0.07692307692307693, 'aroc': 0.7689500280741157}\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.865814696485623, 'f1_score': 0.3225806451612903, 'precision': 0.43478260869565216, 'recall': 0.2564102564102564, 'aroc': 0.7664233576642336}\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.8626198083067093, 'f1_score': 0.0851063829787234, 'precision': 0.25, 'recall': 0.05128205128205128, 'aroc': 0.7518248175182483}\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.8690095846645367, 'f1_score': 0.2545454545454545, 'precision': 0.4375, 'recall': 0.1794871794871795, 'aroc': 0.7698858319296275}\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.865814696485623, 'f1_score': 0.3, 'precision': 0.42857142857142855, 'recall': 0.23076923076923078, 'aroc': 0.7670784203630918}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"Full\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[numerical_col] = scaler.fit_transform(train_df[numerical_col])\n",
    "test_df[numerical_col] = scaler.transform(test_df[numerical_col])\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.8690095846645367, 'f1_score': 0.3050847457627119, 'precision': 0.45, 'recall': 0.23076923076923078, 'aroc': 0.8072244057645518}\n",
      "\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.8626198083067093, 'f1_score': 0.37681159420289856, 'precision': 0.43333333333333335, 'recall': 0.3333333333333333, 'aroc': 0.8153658993075051}\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.8626198083067093, 'f1_score': 0.37681159420289856, 'precision': 0.43333333333333335, 'recall': 0.3333333333333333, 'aroc': 0.8185476324162455}\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.8690095846645367, 'f1_score': 0.4225352112676056, 'precision': 0.46875, 'recall': 0.38461538461538464, 'aroc': 0.8210743028261276}\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.8690095846645367, 'f1_score': 0.4225352112676056, 'precision': 0.46875, 'recall': 0.38461538461538464, 'aroc': 0.822758749766049}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag(\"Scaler\", \"StandardScaler\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "test_data['newage'] = pd.cut(x = test_data['Age'], bins = [17, 30, 42, 61 ], labels = ['18 - 30', '31 - 42', '43 - 60'])\n",
    "\n",
    "\n",
    "test_data['oldyoung'] = pd.cut(x = test_data['Age'], bins = [17, 30, 61], labels = ['young', 'old'])\n",
    "test_data['loyal'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 42], labels = ['fairly', 'loyal'])\n",
    "\n",
    "\n",
    "test_data['masterylevel'] = pd.cut(x = test_data['TotalWorkingYears'], bins = [-1, 3, 10, 421], labels = ['entry', 'intermediate', 'master'])\n",
    "test_data['loyaltylevel'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 10, 42], labels = ['fairly', 'loyal', 'very-loyal'])\n",
    "test_data['dueforprom'] = pd.cut(x = test_data['YearsSinceLastPromotion'], bins = [-1, 5,  16], labels = ['due', 'overdue'])\n",
    "\n",
    "test_dat  = test_data[categorical_col + numerical_col].to_dict(orient='record')\n",
    "X_test = vectorizer.transform(test_dat)\n",
    "\n",
    "prediction = model.predict_proba(X_test)[:,1]\n",
    "dicts = {'id': test_data['id'], 'Attrition': prediction}\n",
    "output_frame = pd.DataFrame(dicts)\n",
    "\n",
    "(output_frame['Attrition'] >=0.5).astype('int').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy=0.4, random_state= 0)\n",
    "new_train_data = train_data.copy()\n",
    "y = new_train_data.pop('Attrition')\n",
    "X_train_new, y = undersample.fit_resample(new_train_data, y)\n",
    "train_x,  test_x,train_y, test_y = train_test_split(X_train_new, y, \n",
    "                                                    test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x[numerical_col] = scaler.fit_transform(train_x[numerical_col])\n",
    "test_x[numerical_col] = scaler.transform(test_x[numerical_col])\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.803030303030303, 'f1_score': 0.59375, 'precision': 0.8636363636363636, 'recall': 0.4523809523809524, 'aroc': 0.8465608465608465}\n",
      "\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.8181818181818182, 'f1_score': 0.6363636363636364, 'precision': 0.875, 'recall': 0.5, 'aroc': 0.8425925925925926}\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.803030303030303, 'f1_score': 0.6060606060606061, 'precision': 0.8333333333333334, 'recall': 0.47619047619047616, 'aroc': 0.8338624338624337}\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.8257575757575758, 'f1_score': 0.6666666666666667, 'precision': 0.8518518518518519, 'recall': 0.5476190476190477, 'aroc': 0.8224867724867726}\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.7954545454545454, 'f1_score': 0.6301369863013699, 'precision': 0.7419354838709677, 'recall': 0.5476190476190477, 'aroc': 0.8052910052910053}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag(\"scaler\", \"StandardScaler\")\n",
    "        mlflow.set_tag(\"sampling\", \"undersampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86        90\n",
      "           1       0.74      0.55      0.63        42\n",
      "\n",
      "    accuracy                           0.80       132\n",
      "   macro avg       0.78      0.73      0.74       132\n",
      "weighted avg       0.79      0.80      0.79       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.803030303030303, 'f1_score': 0.59375, 'precision': 0.8636363636363636, 'recall': 0.4523809523809524, 'aroc': 0.8465608465608465}\n",
      "\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.8181818181818182, 'f1_score': 0.6363636363636364, 'precision': 0.875, 'recall': 0.5, 'aroc': 0.8425925925925926}\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.803030303030303, 'f1_score': 0.6060606060606061, 'precision': 0.8333333333333334, 'recall': 0.47619047619047616, 'aroc': 0.8338624338624337}\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.8257575757575758, 'f1_score': 0.6666666666666667, 'precision': 0.8518518518518519, 'recall': 0.5476190476190477, 'aroc': 0.8224867724867726}\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.7954545454545454, 'f1_score': 0.6301369863013699, 'precision': 0.7419354838709677, 'recall': 0.5476190476190477, 'aroc': 0.8052910052910053}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag(\"scaler\", \"None\")\n",
    "        mlflow.set_tag(\"sampling\", \"undersampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "new_train_data = train_data.copy()\n",
    "y = new_train_data.pop('Attrition')\n",
    "train_x,  test_x,train_y, test_y = train_test_split(new_train_data, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "X_train_new, train_y_new = oversample.fit_resample(X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.7188498402555911, 'f1_score': 0.38028169014084506, 'precision': 0.2621359223300971, 'recall': 0.6923076923076923, 'aroc': 0.759685569904548}\n",
      "\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.7188498402555911, 'f1_score': 0.3623188405797102, 'precision': 0.25252525252525254, 'recall': 0.6410256410256411, 'aroc': 0.7575332210368707}\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.7188498402555911, 'f1_score': 0.37142857142857144, 'precision': 0.25742574257425743, 'recall': 0.6666666666666666, 'aroc': 0.7579075425790754}\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.7092651757188498, 'f1_score': 0.35460992907801414, 'precision': 0.24509803921568626, 'recall': 0.6410256410256411, 'aroc': 0.7583754445068314}\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.7252396166134185, 'f1_score': 0.38571428571428573, 'precision': 0.26732673267326734, 'recall': 0.6923076923076923, 'aroc': 0.763148044169942}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag(\"sampling\", \"oversampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train_new, train_y_new)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x[numerical_col] = scaler.fit_transform(train_x[numerical_col])\n",
    "test_x[numerical_col] = scaler.transform(test_x[numerical_col])\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "\n",
    "X_train_new, train_y_new = oversample.fit_resample(X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'acc': 0.7252396166134185, 'f1_score': 0.41891891891891897, 'precision': 0.28440366972477066, 'recall': 0.7948717948717948, 'aroc': 0.8139621935242373}\n",
      "\n",
      "\n",
      "\n",
      "0.5\n",
      "{'acc': 0.744408945686901, 'f1_score': 0.43661971830985913, 'precision': 0.30097087378640774, 'recall': 0.7948717948717948, 'aroc': 0.820699981283923}\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "{'acc': 0.7476038338658147, 'f1_score': 0.4397163120567376, 'precision': 0.30392156862745096, 'recall': 0.7948717948717948, 'aroc': 0.822197267452742}\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "{'acc': 0.7476038338658147, 'f1_score': 0.4397163120567376, 'precision': 0.30392156862745096, 'recall': 0.7948717948717948, 'aroc': 0.8228523301516002}\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "{'acc': 0.744408945686901, 'f1_score': 0.43661971830985913, 'precision': 0.30097087378640774, 'recall': 0.7948717948717948, 'aroc': 0.8228523301516002}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag('scaler', 'StandardScaler')\n",
    "        mlflow.set_tag(\"sampling\", \"oversampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train_new, train_y_new)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        print(c)\n",
    "        print(output)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "test_data['newage'] = pd.cut(x = test_data['Age'], bins = [17, 30, 42, 61 ], labels = ['18 - 30', '31 - 42', '43 - 60'])\n",
    "test_data['oldyoung'] = pd.cut(x = test_data['Age'], bins = [17, 30, 61], labels = ['young', 'old'])\n",
    "test_data['loyal'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 42], labels = ['fairly', 'loyal'])\n",
    "\n",
    "test_data['masterylevel'] = pd.cut(x = test_data['TotalWorkingYears'], bins = [-1, 3, 10, 421], labels = ['entry', 'intermediate', 'master'])\n",
    "test_data['loyaltylevel'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 10, 42], labels = ['fairly', 'loyal', 'very-loyal'])\n",
    "test_data['dueforprom'] = pd.cut(x = test_data['YearsSinceLastPromotion'], bins = [-1, 5,  16], labels = ['due', 'overdue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[numerical_col] = scaler.transform(test_data[numerical_col])\n",
    "test_dat  = test_data[categorical_col + numerical_col].to_dict(orient='record')\n",
    "X_test = vectorizer.transform(test_dat)\n",
    "\n",
    "prediction = model.predict_proba(X_test)[:,1]\n",
    "dicts = {'id': test_data['id'], 'Attrition': prediction}\n",
    "output_frame = pd.DataFrame(dicts)\n",
    "\n",
    "(output_frame['Attrition'] >=0.5).astype('int').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame.to_csv('../submissions/lnreg001.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
