{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, \n",
    "                             precision_score, recall_score, \n",
    "                             roc_auc_score, classification_report)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/godwin/Documents/Workflow/Attriton/notebooks/mlruns/1', creation_time=1703084781950, experiment_id='1', last_update_time=1703084781950, lifecycle_stage='active', name='Attrition', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../processed_data/attrition.csv')\n",
    "\n",
    "numerical_col = train_data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "numerical_col.remove('id')\n",
    "numerical_col.remove('attrition')\n",
    "\n",
    "categorical_col = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_data, test_size = 0.25, random_state=0)\n",
    "train_y, test_y = train_df.pop('attrition'), test_df.pop(\"attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.8809523809523809, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'aroc': 0.7095135135135135, 'C_value': 0.1}\n",
      "{'acc': 0.8809523809523809, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'aroc': 0.758972972972973, 'C_value': 0.5}\n",
      "{'acc': 0.8809523809523809, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'aroc': 0.7053513513513513, 'C_value': 1}\n",
      "{'acc': 0.8809523809523809, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'aroc': 0.7511351351351351, 'C_value': 3}\n",
      "{'acc': 0.8809523809523809, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0, 'aroc': 0.7087027027027027, 'C_value': 10}\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        output[\"C_value\"] = c\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df[numerical_col] = scaler.fit_transform(train_df[numerical_col])\n",
    "test_df[numerical_col] = scaler.transform(test_df[numerical_col])\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.9, 'f1_score': 0.3823529411764706, 'precision': 0.7222222222222222, 'recall': 0.26, 'aroc': 0.8568108108108108, 'C_value': 0.1}\n",
      "{'acc': 0.8904761904761904, 'f1_score': 0.36111111111111116, 'precision': 0.5909090909090909, 'recall': 0.26, 'aroc': 0.8571351351351352, 'C_value': 0.5}\n",
      "{'acc': 0.888095238095238, 'f1_score': 0.3561643835616438, 'precision': 0.5652173913043478, 'recall': 0.26, 'aroc': 0.8574594594594593, 'C_value': 1}\n",
      "{'acc': 0.888095238095238, 'f1_score': 0.3561643835616438, 'precision': 0.5652173913043478, 'recall': 0.26, 'aroc': 0.8571351351351352, 'C_value': 3}\n",
      "{'acc': 0.888095238095238, 'f1_score': 0.3561643835616438, 'precision': 0.5652173913043478, 'recall': 0.26, 'aroc': 0.8568648648648648, 'C_value': 10}\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"Scaler\", \"StandardScaler\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        output[\"C_value\"] = c\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy=0.4, random_state= 0)\n",
    "new_train_data = train_data.copy()\n",
    "y = new_train_data.pop('attrition')\n",
    "X_train_new, y = undersample.fit_resample(new_train_data, y)\n",
    "train_x,  test_x,train_y, test_y = train_test_split(X_train_new, y, \n",
    "                                                    test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x[numerical_col] = scaler.fit_transform(train_x[numerical_col])\n",
    "test_x[numerical_col] = scaler.transform(test_x[numerical_col])\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.7828571428571428, 'f1_score': 0.5476190476190476, 'precision': 0.6388888888888888, 'recall': 0.4791666666666667, 'aroc': 0.8307086614173228, 'C_value': 0.1}\n",
      "{'acc': 0.7828571428571428, 'f1_score': 0.5681818181818181, 'precision': 0.625, 'recall': 0.5208333333333334, 'aroc': 0.8259514435695539, 'C_value': 0.5}\n",
      "{'acc': 0.7771428571428571, 'f1_score': 0.5617977528089888, 'precision': 0.6097560975609756, 'recall': 0.5208333333333334, 'aroc': 0.8231627296587927, 'C_value': 1}\n",
      "{'acc': 0.7828571428571428, 'f1_score': 0.5681818181818181, 'precision': 0.625, 'recall': 0.5208333333333334, 'aroc': 0.8179133858267716, 'C_value': 3}\n",
      "{'acc': 0.7828571428571428, 'f1_score': 0.5681818181818181, 'precision': 0.625, 'recall': 0.5208333333333334, 'aroc': 0.8154527559055118, 'C_value': 10}\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"scaler\", \"StandardScaler\")\n",
    "        mlflow.set_tag(\"sampling\", \"undersampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train, train_y)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        output[\"C_value\"] = c\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85       127\n",
      "           1       0.62      0.52      0.57        48\n",
      "\n",
      "    accuracy                           0.78       175\n",
      "   macro avg       0.73      0.70      0.71       175\n",
      "weighted avg       0.77      0.78      0.78       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "new_train_data = train_data.copy()\n",
    "y = new_train_data.pop('attrition')\n",
    "train_x,  test_x,train_y, test_y = train_test_split(new_train_data, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "X_train_new, train_y_new = oversample.fit_resample(X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.6761904761904762, 'f1_score': 0.33980582524271846, 'precision': 0.22435897435897437, 'recall': 0.7, 'aroc': 0.7465405405405406, 'C_value': 0.1}\n",
      "{'acc': 0.6547619047619048, 'f1_score': 0.30622009569377995, 'precision': 0.20125786163522014, 'recall': 0.64, 'aroc': 0.7128108108108108, 'C_value': 0.5}\n",
      "{'acc': 0.65, 'f1_score': 0.30331753554502366, 'precision': 0.19875776397515527, 'recall': 0.64, 'aroc': 0.701081081081081, 'C_value': 1}\n",
      "{'acc': 0.65, 'f1_score': 0.30331753554502366, 'precision': 0.19875776397515527, 'recall': 0.64, 'aroc': 0.7185945945945946, 'C_value': 3}\n",
      "{'acc': 0.6166666666666667, 'f1_score': 0.296943231441048, 'precision': 0.18994413407821228, 'recall': 0.68, 'aroc': 0.6805945945945945, 'C_value': 10}\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag(\"sampling\", \"oversampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train_new, train_y_new)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        output[\"C_value\"] = c\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_x[numerical_col] = scaler.fit_transform(train_x[numerical_col])\n",
    "test_x[numerical_col] = scaler.transform(test_x[numerical_col])\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "\n",
    "X_train_new, train_y_new = oversample.fit_resample(X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.7833333333333333, 'f1_score': 0.46153846153846156, 'precision': 0.3277310924369748, 'recall': 0.78, 'aroc': 0.8525945945945946, 'C_value': 0.1}\n",
      "{'acc': 0.7928571428571428, 'f1_score': 0.4790419161676647, 'precision': 0.3418803418803419, 'recall': 0.8, 'aroc': 0.8528648648648649, 'C_value': 0.5}\n",
      "{'acc': 0.7904761904761904, 'f1_score': 0.47619047619047616, 'precision': 0.3389830508474576, 'recall': 0.8, 'aroc': 0.8527027027027027, 'C_value': 1}\n",
      "{'acc': 0.7904761904761904, 'f1_score': 0.47619047619047616, 'precision': 0.3389830508474576, 'recall': 0.8, 'aroc': 0.8533513513513513, 'C_value': 3}\n",
      "{'acc': 0.7904761904761904, 'f1_score': 0.47619047619047616, 'precision': 0.3389830508474576, 'recall': 0.8, 'aroc': 0.8535675675675676, 'C_value': 10}\n"
     ]
    }
   ],
   "source": [
    "for c in(0.1, 0.5, 1, 3, 10):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag('model', 'Logisticreg')\n",
    "        mlflow.set_tag('scaler', 'StandardScaler')\n",
    "        mlflow.set_tag(\"sampling\", \"oversampled\")\n",
    "        mlflow.log_param(\"C\", c)\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(X_train_new, train_y_new)\n",
    "        prediction0 = model.predict_proba(X_val)[:,1]\n",
    "        prediction = model.predict(X_val)\n",
    "        output = {\"acc\":accuracy_score(test_y, prediction), \n",
    "                \"f1_score\":f1_score(test_y, prediction), \n",
    "                \"precision\":precision_score(test_y, prediction), \n",
    "                \"recall\":recall_score(test_y, prediction),\n",
    "                \"aroc\": roc_auc_score(test_y, prediction0)}\n",
    "        mlflow.log_metrics(output)\n",
    "        output[\"C_value\"] = c\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "test_data['newage'] = pd.cut(x = test_data['Age'], bins = [17, 30, 42, 61 ], labels = ['18 - 30', '31 - 42', '43 - 60'])\n",
    "test_data['oldyoung'] = pd.cut(x = test_data['Age'], bins = [17, 30, 61], labels = ['young', 'old'])\n",
    "test_data['loyal'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 42], labels = ['fairly', 'loyal'])\n",
    "\n",
    "test_data['masterylevel'] = pd.cut(x = test_data['TotalWorkingYears'], bins = [-1, 3, 10, 421], labels = ['entry', 'intermediate', 'master'])\n",
    "test_data['loyaltylevel'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 10, 42], labels = ['fairly', 'loyal', 'very-loyal'])\n",
    "test_data['dueforprom'] = pd.cut(x = test_data['YearsSinceLastPromotion'], bins = [-1, 5,  16], labels = ['due', 'overdue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[numerical_col] = scaler.transform(test_data[numerical_col])\n",
    "test_dat  = test_data[categorical_col + numerical_col].to_dict(orient='record')\n",
    "X_test = vectorizer.transform(test_dat)\n",
    "\n",
    "prediction = model.predict_proba(X_test)[:,1]\n",
    "dicts = {'id': test_data['id'], 'Attrition': prediction}\n",
    "output_frame = pd.DataFrame(dicts)\n",
    "output_frame.to_csv('../submissions/lnreg001.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
