{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/godwin/Documents/Workflow/Attriton/notebooks/mlruns/1', creation_time=1701070848011, experiment_id='1', last_update_time=1701070848011, lifecycle_stage='active', name='Attrition', tags={}>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/newtrain1.csv')\n",
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "numerical_col = train_data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "numerical_col.remove('id')\n",
    "numerical_col.remove( 'Attrition')\n",
    "numerical_col.remove('EmployeeCount')\n",
    "numerical_col.remove('StandardHours')\n",
    "train_data = train_data[train_data['TrainingTimesLastYear'] <= 4]\n",
    "train_data = train_data[train_data['TrainingTimesLastYear'] > 0]\n",
    "train_data = train_data[train_data['YearsSinceLastPromotion'] <= 5]\n",
    "train_data = train_data[train_data['YearsWithCurrManager'] <= 13]\n",
    "#numerical_col.remove('PerformanceRating')##########\n",
    "\n",
    "categorical_col = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_col.remove('Over18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = ['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus', 'OverTime','newage', 'masterylevel', 'loyaltylevel', 'oldyoung', 'loyal']\n",
    "numerical_col = ['DailyRate', 'DistanceFromHome',  'Education',  'EnvironmentSatisfaction',\n",
    "            'HourlyRate', 'JobInvolvement', 'JobSatisfaction',  'MonthlyIncome',  'NumCompaniesWorked', 'PerformanceRating',\n",
    "            'RelationshipSatisfaction',  'StockOptionLevel',  'TrainingTimesLastYear',  'WorkLifeBalance',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_data, test_size = 0.25, random_state=0)\n",
    "train_y, test_y = train_df.pop('Attrition'), test_df.pop(\"Attrition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = train_y, feature_names = feature_names)\n",
    "dtest = xgb.DMatrix(X_val, label = test_y, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.set_tag(\"data\", \"original\")\n",
    "        mlflow.set_tag(\"loss\", \"RMSE\")\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtest, 'validation')],\n",
    "            early_stopping_rounds=200\n",
    "            )\n",
    "        prediction0 = booster.predict(dtest)\n",
    "        prediction = (prediction0 >= 0.5).astype('int')\n",
    "        rmse = mean_squared_error(y_true = test_y.astype('float'), y_pred =prediction0,  squared=True)\n",
    "        f1 = f1_score(test_y, prediction)\n",
    "        output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1_score(test_y, prediction), \n",
    "                  \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction), \"rmse\":rmse}\n",
    "        mlflow.log_metrics(output)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "scaler = MinMaxScaler((0,10))\n",
    "train_df[numerical_col] = scaler.fit_transform(train_df[numerical_col])\n",
    "test_df[numerical_col] = scaler.transform(test_df[numerical_col])\n",
    "\n",
    "train_dicts = train_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_df[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = train_y, feature_names = feature_names)\n",
    "dtest = xgb.DMatrix(X_val, label = test_y, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        mlflow.set_tag(\"scaler\", \"standard\")\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtest, 'validation')],\n",
    "            early_stopping_rounds=200\n",
    "            )\n",
    "        prediction0 = booster.predict(dtest)\n",
    "        prediction = (prediction0 >= 0.5).astype('int')\n",
    "        f1 = f1_score(test_y, prediction)\n",
    "        output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1_score(test_y, prediction), \n",
    "                  \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction), }\n",
    "        mlflow.log_metrics(output)\n",
    "\n",
    "    return {'loss': -f1, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\"learning_rate\":0.12156676810099117,\n",
    "               \"max_depth\":\t74,\n",
    "               \"min_child_weight\":18.7107722766818,\n",
    "               \"objective\":\"binary:logistic\",\n",
    "               \"reg_alpha\":0.03791356716583494,\n",
    "               \"reg_lambda\":0.05021397566611148,\n",
    "               \"seed\":42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "    \n",
    "train = xgb.DMatrix(X_train, label=train_y)\n",
    "valid = xgb.DMatrix(X_val, label=test_y)\n",
    "\n",
    "best_params = {\"learning_rate\":0.5252319423885001,\n",
    "              \"max_depth\":60,\n",
    "              \"min_child_weight\":14.511009837774113,\n",
    "              \"objective\":\"binary:logistic\",\n",
    "              \"reg_alpha\":0.024980836694059552,\n",
    "              \"reg_lambda\":\t0.002504555735308145,\n",
    "               \"seed\":42}\n",
    "\n",
    "# mlflow.log_params(best_params)\n",
    "\n",
    "booster = xgb.train(\n",
    "    params=best_params,\n",
    "    dtrain=train,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(valid, 'validation')],\n",
    "    early_stopping_rounds=200\n",
    ")\n",
    "\n",
    "prediction0 = booster.predict(valid)\n",
    "prediction = (prediction0 >=0.5).astype('int')\n",
    "f1 = f1_score(test_y, prediction)\n",
    "output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1_score(test_y, prediction), \n",
    "            \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction)}\n",
    "    #mlflow.log_metrics(output)\n",
    "\n",
    "    # with open(\"../Models/preprocessor.b\", \"wb\") as f_out:\n",
    "    #     pickle.dump(dv, f_out)\n",
    "    # mlflow.log_artifact(\"Models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "    # mlflow.xgboost.log_model(booster, artifact_path=\"models_mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "test_data['newage'] = pd.cut(x = test_data['Age'], bins = [17, 30, 42, 61 ], labels = ['18 - 30', '31 - 42', '43 - 60'])\n",
    "\n",
    "\n",
    "test_data['oldyoung'] = pd.cut(x = test_data['Age'], bins = [17, 30, 61], labels = ['young', 'old'])\n",
    "test_data['loyal'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 42], labels = ['fairly', 'loyal'])\n",
    "\n",
    "\n",
    "test_data['masterylevel'] = pd.cut(x = test_data['TotalWorkingYears'], bins = [-1, 3, 10, 421], labels = ['entry', 'intermediate', 'master'])\n",
    "test_data['loyaltylevel'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 10, 42], labels = ['fairly', 'loyal', 'very-loyal'])\n",
    "test_data['dueforprom'] = pd.cut(x = test_data['YearsSinceLastPromotion'], bins = [-1, 5,  16], labels = ['due', 'overdue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dat  = test_data[categorical_col + numerical_col].to_dict(orient='record')\n",
    "X_test = vectorizer.transform(test_dat)\n",
    "eval_data  = xgb.DMatrix(X_test)\n",
    "\n",
    "prediction = booster.predict(eval_data)\n",
    "dicts = {'id': test_data['id'], 'Attrition': prediction}\n",
    "output_frame = pd.DataFrame(dicts)\n",
    "\n",
    "(output_frame['Attrition'] >=0.5).astype('int').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame.to_csv('../submissions/base007.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = train_data.copy()\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.4, random_state = 0)\n",
    "y = new_train_data.pop('Attrition')\n",
    "X_train_new, y = undersample.fit_resample(new_train_data, y)\n",
    "train_x,  test_x,train_y, test_y = train_test_split(X_train_new, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# train_x[numerical_col] = scaler.fit_transform(train_x[numerical_col])\n",
    "# test_x[numerical_col] = scaler.transform(test_x[numerical_col])\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = train_y, feature_names = feature_names)\n",
    "dtest = xgb.DMatrix(X_val, label = test_y, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.set_tag(\"data\", \"full\")\n",
    "        #mlflow.set_tag(\"scaler\", \"standard\")\n",
    "        mlflow.set_tag(\"sampling\", 'undersampling')\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtest, 'validation')],\n",
    "            early_stopping_rounds=200\n",
    "            )\n",
    "        prediction0 = booster.predict(dtest)\n",
    "        prediction = (prediction0 >= 0.5).astype('int')\n",
    "        aroc = roc_auc_score(test_y,prediction0,)\n",
    "        f1 = f1_score(test_y, prediction)\n",
    "        output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1, \n",
    "                    \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction), \"area_roc\":aroc}\n",
    "        mlflow.log_metrics(output)\n",
    "\n",
    "    return {'loss': -aroc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "    \n",
    "# train = xgb.DMatrix(X_train, label=train_y)\n",
    "# valid = xgb.DMatrix(X_val, label=test_y)\n",
    "\n",
    "# best_params = {\"learning_rate\":0.10361831885254409,\n",
    "#                \"max_depth\":12,\n",
    "#                \"min_child_weight\":14.729488950791687,\n",
    "#                \"objective\":\"binary:logistic\",\n",
    "#                \"reg_alpha\":0.3425942221271259,\n",
    "#                \"reg_lambda\":0.12207176639906649,\n",
    "#                \"seed\":42}\n",
    "\n",
    "# booster = xgb.train(\n",
    "#     params=best_params,\n",
    "#     dtrain=train,\n",
    "#     num_boost_round=1000,\n",
    "#     evals=[(valid, 'validation')],\n",
    "#     early_stopping_rounds=200\n",
    "# )\n",
    "\n",
    "prediction0 = booster.predict(train)\n",
    "prediction = (prediction0 >=0.5).astype('int')\n",
    "f1 = f1_score(train_y, prediction)\n",
    "output = {\"acc\": accuracy_score(train_y, prediction), \"f1_score\": f1, \n",
    "            \"precision\": precision_score(train_y, prediction), \"recall\": recall_score(train_y, prediction), 'area_roc':roc_auc_score(train_y, prediction0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9015151515151515,\n",
       " 'f1_score': 0.8059701492537313,\n",
       " 'precision': 0.8804347826086957,\n",
       " 'recall': 0.7431192660550459,\n",
       " 'area_roc': 0.9562062462040086}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/bct-data-summit/test.csv')\n",
    "\n",
    "test_data['newage'] = pd.cut(x = test_data['Age'], bins = [17, 30, 42, 61 ], labels = ['18 - 30', '31 - 42', '43 - 60'])\n",
    "\n",
    "\n",
    "test_data['oldyoung'] = pd.cut(x = test_data['Age'], bins = [17, 30, 61], labels = ['young', 'old'])\n",
    "test_data['loyal'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 42], labels = ['fairly', 'loyal'])\n",
    "\n",
    "\n",
    "test_data['masterylevel'] = pd.cut(x = test_data['TotalWorkingYears'], bins = [-1, 3, 10, 421], labels = ['entry', 'intermediate', 'master'])\n",
    "test_data['loyaltylevel'] = pd.cut(x = test_data['YearsAtCompany'], bins = [-1, 3, 10, 42], labels = ['fairly', 'loyal', 'very-loyal'])\n",
    "test_data['dueforprom'] = pd.cut(x = test_data['YearsSinceLastPromotion'], bins = [-1, 5,  16], labels = ['due', 'overdue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dat  = test_data[categorical_col + numerical_col].to_dict(orient='record')\n",
    "X_test = vectorizer.transform(test_dat)\n",
    "eval_data  = xgb.DMatrix(X_test)\n",
    "\n",
    "prediction = booster.predict(eval_data)\n",
    "dicts = {'id': test_data['id'], 'Attrition': prediction}\n",
    "output_frame = pd.DataFrame(dicts)\n",
    "\n",
    "(output_frame['Attrition'] >=0.5).astype('int').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frame.to_csv('../submissions/xgb002.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        #mlflow.set_tag(\"scaler\", \"standard\")\n",
    "        mlflow.set_tag(\"sampling\", 'undersampling')\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtest, 'validation')],\n",
    "            early_stopping_rounds=200\n",
    "            )\n",
    "        prediction0 = booster.predict(dtest)\n",
    "        prediction = (prediction0 >= 0.5).astype('int')\n",
    "        rmse = mean_squared_error(y_true = test_y.astype('float'), y_pred =prediction0,  squared=True)\n",
    "        f1 = f1_score(test_y, prediction)\n",
    "        output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1_score(test_y, prediction), \n",
    "                  \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction), \"rmse\":rmse}\n",
    "        mlflow.log_metrics(output)\n",
    "\n",
    "    return {'loss': -f1, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "new_train_data = train_data.copy()\n",
    "y = new_train_data.pop('Attrition')\n",
    "train_x,  test_x,train_y, test_y = train_test_split(new_train_data, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_dicts = train_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "val_dicts = test_x[categorical_col + numerical_col].to_dict(orient='records')\n",
    "vectorizer.fit(train_dicts)\n",
    "feature_names = vectorizer.get_feature_names_out().tolist()\n",
    "\n",
    "X_train = vectorizer.transform(train_dicts)\n",
    "X_val = vectorizer.transform(val_dicts)\n",
    "X_train_new, train_y_new = oversample.fit_resample(X_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = train_y, feature_names = feature_names)\n",
    "dtest = xgb.DMatrix(X_val, label = test_y, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        mlflow.set_tag(\"data\", \"engineered\")\n",
    "        #mlflow.set_tag(\"scaler\", \"standard\")\n",
    "        mlflow.set_tag(\"sampling\", 'oversampling')\n",
    "        mlflow.log_params(params)\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtest, 'validation')],\n",
    "            early_stopping_rounds=200\n",
    "            )\n",
    "        prediction0 = booster.predict(dtest)\n",
    "        prediction = (prediction0 >= 0.5).astype('int')\n",
    "        rmse = mean_squared_error(y_true = test_y.astype('float'), y_pred =prediction0,  squared=True)\n",
    "        f1 = f1_score(test_y, prediction)\n",
    "        output = {\"acc\": accuracy_score(test_y, prediction), \"f1_score\": f1_score(test_y, prediction), \n",
    "                  \"precision\": precision_score(test_y, prediction), \"recall\": recall_score(test_y, prediction), \"rmse\":rmse}\n",
    "        mlflow.log_metrics(output)\n",
    "\n",
    "    return {'loss': -f1, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'binary:logistic',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\"learning_rate\":\t0.1992334055006196,\n",
    "               \"max_depth\":\t71,\n",
    "               \"min_child_weight\":6.647476155770954,\n",
    "               \"objective\":\t\"binary:logistic\",\n",
    "               \"reg_alpha\":\t0.009732448304248409,\n",
    "               \"reg_lambda\":0.01586123080836735,\n",
    "               \"seed\":42}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
